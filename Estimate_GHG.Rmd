---
title: "Net GHG Assessment"
author: "Lauren Mabe"
date: "4-13-2022"
output:
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    theme: journal
    toc: yes
    toc_float: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

This file explains the code of the net GHG analysis used in the study. The GHG analysis was applied to both the K-means and FWADA models at two AD capacities. Cluster Vehicle Miles Traveled (VMT) was calculated using the Traveling Salesman's Problem routing algorithm in the TSP package. Two GHG emissions scenarios were calculated that differ depending on the volume of FW in the AD. Data sources for emissions factors are listed in the code chunks.


# Setup
## Libraries
```{r, warning = FALSE, message = FALSE}
# for spatial data
library(sp)
library(raster)

# for sample data built into package
library(spData)

library(TSP) # for estimating collection route

```

## Data
A spatial points dataset from spData package is used as an example cluster. Each point is randomly assigned a FW_dis value (tons of FW disposed of annually) to substitute for the Food Waste Geography (FWG). The points represent commercial businesses that dispose of FW in Los Angeles County (BUS). The weighted center of the cluster represents the AD (AD).
```{r}

set.seed(666) # for random number generation

# ~~~~~~~~ generate BUS object ~~~~~~~~~~~~~~~~~ #
# Data on 25,357 single family homes sold in Lucas County, Ohio, 1993-1998
data(house)

# create a "FW_dis" column using random numbers from 0:5
house$FW_dis <- runif(nrow(house), 0, 5)

# index column
house$idx <- 1:nrow(house)

# drop the built-in columns
house <- house[,which(names(house) %in% c("idx","FW_dis"))]


# ~~~~~~~~~ generate AD object ~~~~~~~~~~~~~~~~~~~~~~ 
# calculate weighted center for the AD
weightedCenter <- function(cluster1) {
        # get weighted center
        wcx <- weighted.mean(x = cluster1@coords[,1], 
                         w = cluster1$FW_dis)
        
        wcy <- weighted.mean(x = cluster1@coords[,2], 
                         w = cluster1$FW_dis)
        return(c(wcx, wcy))
  } # close weighted center

wc <- weightedCenter(house)

# create sp object
# create dataframe for the AD
coord_df <- data.frame(AD_code = "x1",
                       AD_cap = 55000,
                       FW_dis = sum(house$FW_dis))
# create sp object
wc <- SpatialPointsDataFrame(coords = t(as.matrix(wc)), data = coord_df, proj4string = house@proj4string)


#~~~~~~~~~ cluster~~~~~~~~~~~~~
# create a cluster object to hold AD and BUS points
cluster <- list(AD = wc, BUS = house)


# plot
plot(cluster$BUS, pch = 19, col = "black", main = paste0("sample cluster | num points: ", nrow(house)))
points(cluster$AD, pch = 24, col = "black", bg = "red")


```



# Vehicle Miles Travelled (VMT)
VMT is estimated individually for each cluster in a model's solution using the Travelling Salesmans Algorithm in the TSP package. It is assumed that FW is collected from each business in series, starting at the AD and returning to the AD when all businesses have been visted. Capacity limitations to the truck which would require multiple return trips and increase VMT were ignored. A method to conserve memory was also developed for very large clusters. Given high number of FW generators in the FWG and the use of Monte Carlo simulation, speed of solution was prioritized over route optimally when constructing a collection route. 



## Travelling Salesman's problem
The Travelling Saleman's problem is an NP-hard problem, meaning the number of computational steps required to find a solution cannot be expressed as a polynomial function of the number of points in the problem instance. Heuristic algorithms have been developed to generate routes faster at the expense of optimality. The Nearest Neighbor (NN) algorithm was used as it is the fastest algorithm to solve, simply connecting each point to its nearest neighbor. Post-hoc refinement techniques can be used to decrease the length of the route generated by the NN algorithm, however these were not used.


### Sample TSP - 100 points
A comparison of route length with and without 2-opt refinement using a small portion of the cluster. 2-opt refinement iteratively switches connections between nodes to reduce intersections but increases the time to find a route.
```{r, echo = FALSE}

set.seed(666)

# run TSP on sample of the data
# construct TSP object from some of the coordinates
tsp_obj <- as.ETSP(cluster$BUS@coords[sample(nrow(cluster$BUS), 100),])

# solve TSP w/ nearest neighbor and no-2opt refinement
res <- solve_TSP(tsp_obj, method = "nn", control = c(two_opt = FALSE))

res2 <- solve_TSP(tsp_obj, method = "nn", control = c(two_opt = TRUE))

# plot the output
plot(tsp_obj, res, main = paste0("Nearest Neighbor Algorithm | Route: ", round(tour_length(res, tsp_obj)/5280, 2), " miles"), 
     xlab = "", ylab = "", axes = FALSE, frame.plot = TRUE)

plot(tsp_obj, res2, main = paste0("NN Algorithm w/ 2-opt | Route: ", round(tour_length(res2, tsp_obj)/5280, 2), " miles"),
     xlab = "", ylab = "", axes = FALSE, frame.plot = TRUE)

```



## Large Clusters
Large clusters (over 10,000 points) use too much memory to solve the TSP instance in one go.
These are split into quadrants and TSP was run separately on each quadrant. 
Occasionally, some quadrants are still too large, these are then split again into subquadrants. The bounding box (bbox) of the cluster is used to generate polygons to split the cluster using the over() function.

### Split along cardinal directions
```{r}



# cardinalSplit() splits the given cluster along the cardinal directions
# @param cluster2 - the point cluster - sp object
# @param nsplit2 - the size of the AD that needs to be split
# returns - a list of sp objects - a point cluster for each quadrant
cardinalSplit <- function(cluster2, nsplit2) {
  
  # find the weighted center of the AD again
  # can't just use the AD coordinates if its a second split
  AD_coords2 <- matrix(c(weighted.mean(x = cluster2@coords[,1],
                                         w = cluster2$FW_dis),
                          weighted.mean(x = cluster2@coords[,2],
                                         w = cluster2$FW_dis)),
                       nrow = 2, ncol = 1)
  
  # if the cluster if over 10000 BUS points, it needs to be split into 4
  if (nrow(cluster2) > nsplit2) {
      temp <- cluster2@bbox
      
      # combine the bbox coords with the weighted center
      temp <- cbind(temp, AD_coords2)
      
      rownames(temp) <- c("x", "y")
      colnames(temp) <- c("min", "max", "med")
      


      # make a polygon for each quadrant
      # there's probably a way to do this automatically, but since I only need 4, its by hand
      # the points creating the polygons are the top left (NW) point in that poly, going clockwise
      polys_list <- list()
      
      polys_list[[1]] <- Polygon(matrix(c(temp["x", "min"], temp["x", "med"],
                                          temp["x", "med"], temp["x", "min"], 
                                          temp["y", "max"], temp["y", "max"],
                                          temp["y", "med"], temp["y", "med"]),
                                 nrow = 4, ncol = 2))
      polys_list[[1]] <- Polygons(list(polys_list[[1]]), 1)
      polys_list[[1]] <- SpatialPolygons(list(polys_list[[1]]))
      proj4string(polys_list[[1]]) <- house@proj4string
      
      polys_list[[2]] <- Polygon(matrix(c(temp["x", "med"], temp["x", "max"],
                                          temp["x", "max"], temp["x","med"],
                                          temp["y", "max"], temp["y", "max"],
                                          temp["y", "med"], temp["y", "med"]),
                                        nrow = 4, ncol = 2))
      polys_list[[2]] <- Polygons(list(polys_list[[2]]), 1)
      polys_list[[2]] <- SpatialPolygons(list(polys_list[[2]]))
      proj4string(polys_list[[2]]) <- house@proj4string
      
      polys_list[[3]] <- Polygon(matrix(c(temp["x", "med"], temp["x", "max"],
                                          temp["x", "max"], temp["x", "med"],
                                          temp["y", "med"], temp["y", "med"],
                                          temp["y", "min"], temp["y", "min"]),
                                        nrow = 4, ncol = 2))
      polys_list[[3]] <- Polygons(list(polys_list[[3]]), 1)
      polys_list[[3]] <- SpatialPolygons(list(polys_list[[3]]))
      proj4string(polys_list[[3]]) <- house@proj4string
      
      polys_list[[4]] <- Polygon(matrix(c(temp["x", "min"], temp["x", "med"],
                                          temp["x", "med"], temp["x", "min"],
                                          temp["y", "med"], temp["y", "med"],
                                          temp["y", "min"], temp["y", "min"]),
                                        nrow = 4, ncol = 2))
      polys_list[[4]] <- Polygons(list(polys_list[[4]]), 1)
      polys_list[[4]] <- SpatialPolygons(list(polys_list[[4]]))
      proj4string(polys_list[[4]]) <- house@proj4string
      
      
      # Separate cluster into quadrants
      # create a list w/ 4 different groups of points, one for each cluster using over()
      points_list <- lapply(polys_list, function(x) {cluster2$split <- over(cluster2, x)
                                                     cluster2[is.na(cluster2$split) == FALSE,]})

      names(points_list) <- c("1", "2", "3", "4")
  
  # if none of the points need to be split,
  # just make a list with the whole cluster
  # needs to be a list, thats whats expected in function later
  } else {
    
    points_list <- list(cluster)
    names(points_list) <- c("all")
    
  } # close if/else
  
     return(points_list)
      
} # close cardinalSplit()

# run cardinalSplit() over the cluster
points_list <- cardinalSplit(cluster$BUS, nsplit2 = 10000)


```

```{r, echo = FALSE}


# bounding box sp object
# get bounding box
  bb <- cluster$BUS@bbox
  
  # change rownames to remember which one is which
  rownames(bb) <- c("EW", "NS")
 

# make an sp object of bbox for plotting
bbox_sp <- as.matrix(rbind(c(bb[1,1], bb[2,2]), # NW
                 c(bb[1,2], bb[2,2]), # NE
                 c(bb[1,2], bb[2,1]), # SE
                 c(bb[1,1], bb[2,1]))) # SW
colnames(bbox_sp) <- c("x_coord", "y_coord")


bbox_sp <- Polygon(bbox_sp)
bbox_sp <- Polygons(list(bbox_sp), 1)
bbox_sp <- SpatialPolygons(list(bbox_sp))
bbox_sp@proj4string <- house@proj4string


# spatial lines object to plot the quadrants
lx <- SpatialLines(list(Lines(list(Line(rbind(c(bbox_sp@bbox['x','min'], cluster$AD@coords[,2]), 
                                             c(bbox_sp@bbox['x','max'], cluster$AD@coords[,2])))), 1)))


ly <- SpatialLines(list(Lines(list(Line(rbind(c(cluster$AD@coords[,1], bbox_sp@bbox['y','min']), 
                                             c(cluster$AD@coords[,1], bbox_sp@bbox['y','max'])))), 1)))


{plot(cluster$BUS, pch = 19, col = "grey", main = "cluster quadrants")
points(points_list[[1]], pch = 19, col = "blue")
points(points_list[[2]], pch = 19, col = "red")
points(points_list[[3]], pch = 19, col = "yellow")
points(points_list[[4]], pch = 19, col = "green")
plot(bbox_sp, add = TRUE, lwd = 2)
plot(lx, add = TRUE, lwd = 2)
plot(ly, add = TRUE, lwd = 2)
points(cluster$AD, pch = 24, col = "black", bg = "red")}

```

### Subquadrants if needed
If the number of points in a quadrant still exeed 10,000 points, it is split again.
Note: threshold to split is from 10,000 points to 7,000 points for this example as the sample cluster does not need subquadrants with default settings
```{r}

# run through the list, get size of each list
num_points <- lapply(points_list, function(x) nrow(x))

# if needed, split the ones that need to be split again
# remove the original from the points_list
# and add the new split clusters back into the points_list
if (sum(num_points > 7000) > 0) {
  points_list2 <- lapply(which(num_points > 7000),
                         function(x) cardinalSplit(points_list[[x]], nsplit2 = 7000))
  points_list2 <- unlist(points_list2)
  
  # manually change the names of points_list2 if they came from
  # NW or SW quadrants
  # this will allow us to order them the way we want and the TSP will make a nice loop
  names(points_list2)[grep("1.", names(points_list2))] <- c("1.3", "1.4", "1.1", "1.2")
  names(points_list2)[grep("4.", names(points_list2))] <- c("4.3", "4.4", "4.1", "4.2")
  
  points_list <- points_list[-which(num_points > 7000)]
  points_list <- c(points_list, points_list2)
} # close if



```

```{r, echo = FALSE}

{plot(cluster$BUS, pch = 19, col = "grey",
      main = "NE quadrant split to subquadrants")
points(points_list[[1]], pch = 19, col = "blue")
points(points_list[[2]], pch = 19, col = "red")
points(points_list[[3]], pch = 19, col = "yellow")
points(points_list[[4]], pch = 19, col = "green")
points(points_list[[5]], pch = 19, col = "orange")
points(points_list[[6]], pch = 19, col = "purple")
points(points_list[[7]], pch = 19, col = "hot pink")
plot(bbox_sp, add = TRUE, lwd = 2)
plot(lx, add = TRUE, lwd = 2)
plot(ly, add = TRUE, lwd = 2)
points(cluster$AD, pch = 24, col = "black", bg = "red")}

```

## Generating a route
If cluster has been split into quadrants, the TSP function is ran on each subquadrant individually. The quadrants are connected in a prespecified order that forms a clockwise loop to estimate VMT. This plot shows the order of all possible subquadrants. A function called run_TSP() was developed to generate the route. Sometimes, splitting a cluster into quadrants results in an empty quadrant, if the cluster is not of uniform shape. Empty clusters are skipped over if necessary. 
```{r,echo=FALSE}

lab <- data.frame(x = c(40,10,10,40,60,90,90,60,60,90,90,60,40,10,10,40),
                  y = c(60,60,90,90,90,90,60,60,40,40,10,10,10,10,40,40),
                  label = 1:16)


# show the quadrants/subquadrant diagram
plot(1:100, 1:100, 
     pch = 19, col = "white",
     main = "Quadrant order for run_TSP()", 
     xlab = NA, ylab = NA, 
     axes = FALSE, frame.plot = TRUE) # blank graph
points(lab[,1], lab[,2], 
       pch = 19, col = "white")
text(lab[,1], lab[,2], lab[,3], cex=1, col="black")
abline(v = 50, lwd = 2)
abline(h = 50, lwd = 2)
abline(h = 75, lwd = 1)
abline(v = 75, lwd = 1)
abline(h = 25, lwd = 1)
abline(v = 25, lwd = 1)


```

```{r}

# ~~~~~~~~ run_TSP() function ~~~~~~~~~~~~~
# run_TSP() runs the TSP problem over the given point cluster
# @param clust - the point cluster - sp object
# @param close_pnt - the point used to find the start point - coords
# returns - the tour length in miles
run_TSP <- function(clust, close_pnt) {
  print(paste0("clust size ", nrow(clust@coords)))
  
  # sometimes splitting the cluster returns an empty quadrant
  # if that happens, return a 0 for the length
  if (length(clust@coords) > 0) {
    
    # get the starting point for the TSP, it is closest to the
    d <- pointDistance(close_pnt, clust@coords, lonlat = FALSE)
    start_idx <- which(d == min(d))
    
    # get ETSP object with cluster, solve it, find tour length
    tsp_obj <- as.ETSP(clust@coords)
    tour <- solve_TSP(tsp_obj, method = "nn", control = c(start = start_idx))
    
    # length of tour + distance to travel between quadrants
    tour_dist <- tour_length(tour, tsp_obj)/5280
    tour_dist <- tour_dist + (min(d)/5280)
    
    # return the ending points idx
    tour <- as.integer(tour)   
    end_idx <- tour[length(tour)]
    
    res <- list(end_idx = end_idx, tour_dist = tour_dist)
 
   } else {

    res <- list(end_idx = NA, tour_dist = 0)
    
  } # close if else
  
  return(res)
} # close run_TSP()

# ~~~~~~~~~~~~~~~~~~~~~ find VMT of subquadrants ~~~~~~~~~~~

 # order the list by their name to get circle to go the right way
points_list <- points_list[order(names(points_list))]

# the first time its ran, start at the point closest to the cluster AD
close2 <- cluster$AD@coords

# empty lists
end_idx_list <- list()
end_sp <- list()
route_dist <- list()

# run the TSP over each subquadrant in a for loop
for (i in 1:length(points_list)) {

  temp <- run_TSP(clust = points_list[[i]], close_pnt = close2)

  route_dist[[i]] <- temp$tour_dist

  
  # if the quadrant is empty, duplicate the end SP of the previous quadrant
  # if the NW quadrant is empty (first evaluated), end sp is the AD
  # this is a lot of ifs....maybe should have split on PD to prevent empty quadrants??
  if (is.na(temp$end_idx) == TRUE) {
      if (i == 1) {
        end_sp[[i]] <- SpatialPoints(AD_coords, cluster@proj4string)
      } else {
         end_sp[[i]] <- end_sp[[i-1]]
      } # close inner if/else
  } else {
    end_sp[[i]] <- points_list[[i]][temp$end_idx,]
  } # close ifelse

  # if the cluster is empty, the end_idx wil return as NA
  # in that case, dont change the close2 point
  if (is.na(temp$end_idx) == TRUE) {
    close2 <- close2
  } else {
   # reset the close2 variable as the end point
    close2 <- points_list[[i]][temp$end_idx,]
  } # close ifelse
} # close for loop

# add tour lengths together
route_dist <- do.call(sum, route_dist)

# also add the distance from the last stop back to the AD
d <- pointDistance(cluster$AD@coords, end_sp[[length(end_sp)]]@coords, lonlat = FALSE)
route_dist <- route_dist + (d/5280) # and convert feet to miles

print(paste0("Total Collection Route: ", round(route_dist, 2), " miles"))

```


# Estimate GHG emissions
Net GHG emissions are estimated for a single AD at a time. GHG emissions from collection are a function of the VMT derived from run_TSP() and the weight of FW hauled and represent positive emissions. FW was assumed to be collected weekly with a linear relationship between VMT and weight of FW hauled as FW is collected along the route. GHG credits to the system are considered negative GHGs. Sources of these negative emissions are avoided landfill gas (LFG) emissions and those associated with the use of the AD byproducts. This includes substitution of natural gas (NG) with biogas and the use of digestate as nitrogen (N) fertilizer. Total emissions for the cluster are shown in kg CO2eq.

Two AD operational scenarios are evaluated which differ based on the amount of FW allocated to the AD. In Scenario 1 (Less Efficient AD), a discount factor is applied to biogas generation if the amount of FW in a cluster is greater than the AD capacity. This discount factor accounts for the partial digestion of FW due to an increased organic loading rate (OLR) of the AD. Scenario 2 (Larger AD) models a situation where all FW is digested at optimal rates, as if a larger AD had been placed within the cluster.

## Set up constants
These are emissions factors and other constants. Data sources are listed with each variable in the code.
```{r}


#~~~~~~~ Unit Conversions ~~~~~~~~~~~#
# collection/hauling distance
cluster$AD$collect_dist_mi <- route_dist

# convert [tons FW] into [kg FW]
cluster$AD$AD_cap_kg <- cluster$AD$AD_cap * 907.185
cluster$AD$FW_dis_kg <- cluster$AD$FW_dis * 907.185


#~~~~~~ Set up constants ~~~~~~~~~~~~#

# nitrogen in feedstock - [kg]
  # assuming N content is 2.8% of dry matter (DM)
  # assuming DM content 18.1% wet weight
  # Source: Fisgativa et. al. (2016)
  cluster$AD$N_fdstck <- cluster$AD$FW_dis_kg * 0.181 * 0.028

# Avoided LFG emissions [kgCO2e/kg waste]
  # Food waste avoided landfill emissions factor [0.39 MTCO2e/short ton waste]
  ### Source: CCI Emissions Factor database - Food waste tab/Landfill tab. They cite:
  ### "CARB Method for Estimating Greenhouse Gas Emission Reductions from Diversion
  ###   of Organic Waste from Landfills to Compost Facilities (2017)"
  GHG_FLW_prevent <- 0.39 * 1000*2.2/2000
  
# Avoided Nitrogen fertilizer production - [kg CO2e/kg of N]
  # Source: Moller 2009
  avoid_Nfert_ef <- 8.9
  
# Transportation (Medium & Heavy Duty Vehicles Diesel Vehicles) - [kg CO2e per kg/mi]
  ## Emissions factors: CO2 [10.21 kg CO2e/gal fuel], CH4 & N20 [0.0000051/0.0000048 kg CO2e/mile]
  ### Source: https://www.epa.gov/sites/default/files/2018-03/documents/emission-factors_mar_2018_0.pdf
  ## Truck MPG: [0.22727 gal/mile]
  ### Source: https://www.tandfonline.com/doi/pdf/10.1080/10962247.2014.990587?needAccess=true
  ## Truck capacity: [25,000lbs/11340kg]
  ### Source: https://refusetrucks.scrantonmfg.com/automated-side-loader/roto-pac 
  ## GWP 100 - IPCC
  GHG_fuel_use <- ((0.22727 * 10.21) + (0.0000051 * 25) + (0.0000048 * 298))/11340

# Avoided natural gas (NG) consumption emissions factor - [kg CO2e/m3 of NG]
  # Citation: 53.12 from https://www.eia.gov/environment/emissions/co2_vol_mass.php, and 28.32 m3 per tcf
  avoid_NG_ef <- 53.12/28.32


  
```

## Calculate Equal GHG components
Avoided landfill gas, nitrogen fertilizer displacement, and collection GHGs are dependent on the amount of FW diverted to the AD. In this study, the entire FWG is assumed to be diverted and therefore is the same in both AD operational scenarios.
```{r}

# avoided landfill gas by FW diversion [kg CO2e/year]
LFG_avoid <- cluster$AD$FW_dis_kg * GHG_FLW_prevent

# nitrogen fertilizer substitution for AD digestate (avoided N production) [kg CO2e/year]
  # this is not dependent on digestor efficiency, nitrogen in FW does not change
avoid_N_CO2e <-  cluster$AD$N_fdstck * avoid_Nfert_ef

# Transport GHGs [kg CO2e/year]
  # GHG emissions of a weekly collection route is calculated.
  # the weight transported is halved to model linear increasing FW along collection route
  # this is then multipled by 52 to get yearly collection GHGs to match units of FWG (tons/year)
  # capacity limitations to the truck are ignored
CO2e_collect <-  52 * (((cluster$AD$FW_dis_kg/52)/2) * cluster$AD$collect_dist_mi * GHG_fuel_use)

```


## Scenario 1 (Less Efficient AD)
Scenario 1, all FW goes to digestor, but the AD is less efficient if overfilled, leading to reduced biogas generation and resulting negative emissions. A discount factor that accounts for the faster throughput of FW through the AD is applied to methane generation. This discount is based on the Organic Loading Rate of the AD.
```{r}

# ~~~~~~ Organic Loading Rate ~~~~~~~~~~~
# For non-overfilled digesters, baseline assumptions:
# OLR_base = digester size (55,000 ton; 10,000 ton; 5,000 ton) / 365 days = [ton/days] (OLR - Organic loading rate)
# example:  55,000 ton / 365 days = 150.7 ton/day
# Discount Factor = 1

# Overfilled Digesters:
# OLR_new = tons / 365 days
# example: 65,000 ton/365 days = 178 ton/day
# Discount Factor (DF) = OLR_base / OLR_new
# example:  150.7 ton/day / 178 ton/day = 0.85

# Methane Production Calculation - add discount for all calculations
# AD_cap_kg*0.26*0.45*DF
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  

# Base organic loading rate (daily AD capacity) [kg/day]
OLR_base <- cluster$AD$AD_cap_kg[1] / 365

# Methane generation by AD - [m3]
  # Function of volatile solids in food waste
  # assume 26% volatile solids (Zhang et al 2007, Fisgativa et al 2016)
  # assume  0.45 m3 ch4/kg VS (Bong et al 2018, Fisgativa et al 2016)
  # if FW allocated to AD higher than AD capacity, use a discount factor of OLR_base/OLR_new
  # otherwise, discount factor is 1
cluster$AD$methane.1 <- ifelse(cluster$AD$FW_dis_kg > cluster$AD$AD_cap_kg, 
                               cluster$AD$FW_dis_kg * 0.26 * 0.45 * (OLR_base / (cluster$AD$FW_dis_kg/365)),
                               cluster$AD$FW_dis_kg * 0.26 * 0.45 * 1)

# assuming 60% methane is NG
  # [m3]
  cluster$AD$NG_equiv <- cluster$AD$methane.1 * 0.6

# Avoided GHGs from Natural Gas due to biogas generation [kg CO2e/year]
  # natural gas = 60% methane
avoid_NG_CO2e.1 <-  cluster$AD$NG_equiv * avoid_NG_ef
                            


# Scenario 1 total GHG emissions for the sample cluster
# collection GHGs (the postitive emissions) - all other emissions (GHG credits/negative emissions)
Scen1_GHG_CO2e_kg <- CO2e_collect - LFG_avoid - avoid_N_CO2e - avoid_NG_CO2e.1

print(paste0("Scen 1 GHG for sample cluster: ", round(Scen1_GHG_CO2e_kg, 2), " kg CO2e/year"))
```


## Scenario 2 (Larger AD)
Scenario 2 models an ideal situation where all FW is digested at maximum efficiency with no discount factor applied to biogas generation. If the AD is allocated FW over its capacity, it is assumed a larger AD is placed there that can run at maximum efficiency.
```{r}

# Same data sources as above

# Methane generation by AD - [m3]
cluster$AD$methane.2 <- cluster$AD$FW_dis_kg * 0.26 * 0.45
  
# natural gas equivalent - [m3]
cluster$AD$NG_equiv.2 <- cluster$AD$methane.2 * 0.6
  
# avoided natural gas production [kg C02e / year]
avoid_NG_CO2e.2 = cluster$AD$NG_equiv.2 * avoid_NG_ef


# Scenario 2 total GHG production
Scen2_GHG_CO2e_kg <- CO2e_collect - LFG_avoid - avoid_N_CO2e - avoid_NG_CO2e.2


print(paste0("Scen 2 GHG for sample cluster: ", round(Scen2_GHG_CO2e_kg, 2), " kg CO2e/year"))


```

