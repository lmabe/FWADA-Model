---
title: "Calculate_GHG"
author: "Lauren Mabe"
date: "11/30/2021"
output: 
  html_document:
    theme: journal
    toc: true
    toc_float: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file shows how GHG emissions are calculated for a model's solution


Vehicle Miles Travelled (VMT) is calculated using the Travelling Salesman's Problem routing algorithm, 
VMT is used to calculate GHG emissions associated with collection and transportation.  

Two scenarios are used to estimate GHG emissions of the system. These differ in biogas generation.    
- Scenario 1 (Less Efficient AD) uses a discount factor to reduce efficiency of biogas production if AD is overfilled  
- Scenario 2 (Larger AD) estimates GHG emissions of a system where all ADs are at maximum efficiency

Data sources for GHG emissions are also shown

# Setup
## Load libraries
```{r, warning = FALSE, message = FALSE}
# for spatial data
library(sp)
library(raster)

# for sample data built into package
library(spData)


library(TSP) # for estimating collection route

```

## Load data
The data loaded here is built-in spatial points dataset from the spData package.
It will fill in for the "FW Geography" (FWG) dataset that represents FW disposal in LA County used in the study.  

Each point has been given a random "FW_dis" (tons FW disposed annually) value to work with the model's functions.  

In this file, the entire house dataset will serve as a single cluster. 
This, and the weighted center representing the AD are in a single variable called "cluster". 
This variable is a named list which holds the FW points (BUS) and the weighted center (AD). 
This type of variable/object is used throughout the model, and I call it a "kmeans_result"
```{r}

set.seed(666) # for random number generation

# ~~~~~~~~ Sample cluster ~~~~~~~~~~~~~~~~~ #
# Data on 25,357 single family homes sold in Lucas County, Ohio, 1993-1998
# we are pretending they are commercial businesses, not houses
data(house)

# create a "FW_dis" column using random numbers from 0:5
# this will simulate the "FW Geography" dataset
house$FW_dis <- runif(nrow(house), 0, 5)

house$idx <- 1:nrow(house)

# drop the built-in columns
house <- house[,which(names(house) %in% c("idx","FW_dis"))]



# calculate weighted center for the AD
weightedCenter <- function(cluster1) {
        # get weighted center
        wcx <- weighted.mean(x = cluster1@coords[,1], 
                         w = cluster1$FW_dis)
        
        wcy <- weighted.mean(x = cluster1@coords[,2], 
                         w = cluster1$FW_dis)
        return(c(wcx, wcy))
  } # close weighted center
wc <- weightedCenter(house)

# create sp object
# create dataframe for the AD
coord_df <- data.frame(AD_code = "x1",
                       AD_cap = 55000,
                       FW_dis = sum(house$FW_dis))
# create sp object
wc <- SpatialPointsDataFrame(coords = t(as.matrix(wc)), data = coord_df, proj4string = house@proj4string)

# create a FW_kmeans object (named list)
cluster <- list(AD = wc, BUS = house)

# plot
plot(cluster$BUS, pch = 19, col = "black", main = paste0("sample cluster | num points: ", nrow(house)))
points(cluster$AD, pch = 24, col = "black", bg = "red")


```



# Estimate VMT
Vehicle Miles Travelled (VMT) is calcuated separately for each cluster.
One route is calculated which assumes the collection truck starts at the AD in the centroid,
travels to each FW point in series,
then returns to the AD.
Capacity limitations to the truck are ignored.  

This method is used to estimate VMT for a weekly collection route. 
When calculating GHG emissions, all (weekly) FW in the cluster is assumed to travel along the entire route. 
Updates to the model would include the variable (increasing) amount of FW being transported along the route as the truck picks up more from each successive point.  

## Travelling Salesman's problem
The nearest neighbor algorithm in the TSP package was used to construct the route.
This is the simplest algorithm, connecting each point to its closest neighbor until all points are connected. 
It is a quick to solve algorithm, but generally does not generate the shortest route.
2-3opt refinement can be used to optimize the route, however this also increases solve-time and was not used. 

### Sample TSP - 100 points
```{r, echo = FALSE}

set.seed(666)

# run TSP on sample of the data
# construct TSP object from some of the coordinates
tsp_obj <- as.ETSP(cluster$BUS@coords[sample(nrow(cluster$BUS), 100),])

# solve TSP w/ nearest neighbor and no-2opt refinement
res <- solve_TSP(tsp_obj, method = "nn", control = c(two_opt = FALSE))

# plot the output
plot(tsp_obj, res, main = paste0("Sample Tour (100 points) | Length: ", round(tour_length(res, tsp_obj)/5280, 2), " miles"))



```

A sample route of the nearest neighbor algorithm with no post-hoc route refinement.

## Separate Large clusters
Large clusters (over 10,000 points) use too much memory to solve the TSP instance.
These are split into quadrants and TSP was run separately on each quadrant. 
Occasionally, some quadrants are still too large, these are then split again into subquadrants. 

### Split along cardinal directions
```{r}



# cardinalSplit() splits the given cluster along the cardinal directions
# @param cluster2 - the point cluster - sp object
# @param nsplit2 - the size of the AD that needs to be split
# returns - a list of sp objects - a point cluster for each quadrant
cardinalSplit <- function(cluster2, nsplit2) {
  
  # find the weighted center of the AD
  # can't just use the AD coordinates if its a second split
  AD_coords2 <- matrix(c(weighted.mean(x = cluster2@coords[,1],
                                         w = cluster2$FW_dis),
                          weighted.mean(x = cluster2@coords[,2],
                                         w = cluster2$FW_dis)),
                       nrow = 2, ncol = 1)
  
  # if the cluster if over 10000 BUS points, it needs to be split into 4
  if (nrow(cluster2) > nsplit2) {
      poop <- cluster2@bbox
      
      # combine the bbox coords with the weighted center
      poop <- cbind(poop, AD_coords2)
      
      rownames(poop) <- c("x", "y")
      colnames(poop) <- c("min", "max", "med")
      


      # make a polygon for each quadrant
      # there's probably a way to do this automatically, but since I only need 4, its by hand
      # the points creating the polygons are the top left (NW) point in that poly, going clockwise
      # (should have started at center point, oh well)
      polys_list <- list()
      
      polys_list[[1]] <- Polygon(matrix(c(poop["x", "min"], poop["x", "med"],
                                          poop["x", "med"], poop["x", "min"], 
                                          poop["y", "max"], poop["y", "max"],
                                          poop["y", "med"], poop["y", "med"]),
                                 nrow = 4, ncol = 2))
      polys_list[[1]] <- Polygons(list(polys_list[[1]]), 1)
      polys_list[[1]] <- SpatialPolygons(list(polys_list[[1]]))
      proj4string(polys_list[[1]]) <- house@proj4string
      
      polys_list[[2]] <- Polygon(matrix(c(poop["x", "med"], poop["x", "max"],
                                          poop["x", "max"], poop["x","med"],
                                          poop["y", "max"], poop["y", "max"],
                                          poop["y", "med"], poop["y", "med"]),
                                        nrow = 4, ncol = 2))
      polys_list[[2]] <- Polygons(list(polys_list[[2]]), 1)
      polys_list[[2]] <- SpatialPolygons(list(polys_list[[2]]))
      proj4string(polys_list[[2]]) <- house@proj4string
      
      polys_list[[3]] <- Polygon(matrix(c(poop["x", "med"], poop["x", "max"],
                                          poop["x", "max"], poop["x", "med"],
                                          poop["y", "med"], poop["y", "med"],
                                          poop["y", "min"], poop["y", "min"]),
                                        nrow = 4, ncol = 2))
      polys_list[[3]] <- Polygons(list(polys_list[[3]]), 1)
      polys_list[[3]] <- SpatialPolygons(list(polys_list[[3]]))
      proj4string(polys_list[[3]]) <- house@proj4string
      
      polys_list[[4]] <- Polygon(matrix(c(poop["x", "min"], poop["x", "med"],
                                          poop["x", "med"], poop["x", "min"],
                                          poop["y", "med"], poop["y", "med"],
                                          poop["y", "min"], poop["y", "min"]),
                                        nrow = 4, ncol = 2))
      polys_list[[4]] <- Polygons(list(polys_list[[4]]), 1)
      polys_list[[4]] <- SpatialPolygons(list(polys_list[[4]]))
      proj4string(polys_list[[4]]) <- house@proj4string
      
      
      # Get the points in each quadrant
      # create a list w/ 4 different groups of points, one for each cluster using over()
      points_list <- lapply(polys_list, function(x) {cluster2$split <- over(cluster2, x)
                                                     cluster2[is.na(cluster2$split) == FALSE,]})

      names(points_list) <- c("1", "2", "3", "4")
  
  # if none of the points need to be split,
  # just make a list with the whole cluster
  # needs to be a list, thats whats expected
  } else {
    
    points_list <- list(cluster)
    names(points_list) <- c("all")
    
  } # close if/else
  
     return(points_list)
      
} # close cardinalSplit()

# run cardinalSplit() over the cluster
points_list <- cardinalSplit(cluster$BUS, nsplit2 = 10000)

str(points_list)

```

```{r, echo = FALSE}


# bounding box sp object
# get bounding box
  bb <- cluster$BUS@bbox
  
  # change rownames to remember which one is which
  rownames(bb) <- c("EW", "NS")
 

# make an sp object of bbox for plotting
bbox_sp <- as.matrix(rbind(c(bb[1,1], bb[2,2]), # NW
                 c(bb[1,2], bb[2,2]), # NE
                 c(bb[1,2], bb[2,1]), # SE
                 c(bb[1,1], bb[2,1]))) # SW
colnames(bbox_sp) <- c("x_coord", "y_coord")


bbox_sp <- Polygon(bbox_sp)
bbox_sp <- Polygons(list(bbox_sp), 1)
bbox_sp <- SpatialPolygons(list(bbox_sp))
bbox_sp@proj4string <- house@proj4string


# spatial lines object to plot the quadrants
lx <- SpatialLines(list(Lines(list(Line(rbind(c(bbox_sp@bbox['x','min'], cluster$AD@coords[,2]), 
                                             c(bbox_sp@bbox['x','max'], cluster$AD@coords[,2])))), 1)))


ly <- SpatialLines(list(Lines(list(Line(rbind(c(cluster$AD@coords[,1], bbox_sp@bbox['y','min']), 
                                             c(cluster$AD@coords[,1], bbox_sp@bbox['y','max'])))), 1)))


{plot(cluster$BUS, pch = 19, col = "grey", main = "cluster quadrants")
points(points_list[[1]], pch = 19, col = "blue")
points(points_list[[2]], pch = 19, col = "red")
points(points_list[[3]], pch = 19, col = "yellow")
points(points_list[[4]], pch = 19, col = "green")
plot(bbox_sp, add = TRUE, lwd = 2)
plot(lx, add = TRUE, lwd = 2)
plot(ly, add = TRUE, lwd = 2)
points(cluster$AD, pch = 24, col = "black", bg = "red")}

```


### split into subquadrants if needed
Note: threshold to split is from 10,000 points to 7,000 points for this example as the current cluster does not need subquadrants
```{r}

# run through the list, get size of each list
num_points <- lapply(points_list, function(x) nrow(x))

# if needed, split the ones that need to be split again
# remove the original from the points_list
# and add the new split clusters back into the points_list
if (sum(num_points > 7000) > 0) {
  points_list2 <- lapply(which(num_points > 7000),
                         function(x) cardinalSplit(points_list[[x]], nsplit2 = 7000))
  points_list2 <- unlist(points_list2)
  
  # manually change the names of points_list2 if they came from
  # NW or SW quadrants
  # this will allow us to order them the way we want and the TSP will make a nice loop
  names(points_list2)[grep("1.", names(points_list2))] <- c("1.3", "1.4", "1.1", "1.2")
  names(points_list2)[grep("4.", names(points_list2))] <- c("4.3", "4.4", "4.1", "4.2")
  
  points_list <- points_list[-which(num_points > 7000)]
  points_list <- c(points_list, points_list2)
} # close if



```

```{r, echo = FALSE}

{plot(cluster$BUS, pch = 19, col = "grey", main = "cluster quadrants")
points(points_list[[1]], pch = 19, col = "blue")
points(points_list[[2]], pch = 19, col = "red")
points(points_list[[3]], pch = 19, col = "yellow")
points(points_list[[4]], pch = 19, col = "green")
points(points_list[[5]], pch = 19, col = "orange")
points(points_list[[6]], pch = 19, col = "purple")
points(points_list[[7]], pch = 19, col = "hot pink")
plot(bbox_sp, add = TRUE, lwd = 2)
plot(lx, add = TRUE, lwd = 2)
plot(ly, add = TRUE, lwd = 2)
points(cluster$AD, pch = 24, col = "black", bg = "red")}

```

NE quadrant split into subquadrants.
(need to draw those lines)

## Estimate VMT
The TSP package is used to generate a route. 
All quadrants/subquadrants are connected in a prespecified order (shown below) that forms a clockwise loop to estimate VMT.
```{r,echo=FALSE}

lab <- data.frame(x = c(40,10,10,40,60,90,90,60,60,90,90,60,40,10,10,40),
                  y = c(60,60,90,90,90,90,60,60,40,40,10,10,10,10,40,40),
                  label = 1:16)


# show the quadrants/subquadrant diagram
plot(1:100, 1:100, 
     pch = 19, col = "white",
     main = "Quadrant order", 
     xlab = NA, ylab = NA, 
     axes = TRUE) # blank graph
points(lab[,1], lab[,2], 
       pch = 19, col = "white")
text(lab[,1], lab[,2], lab[,3], cex=1, col="black")
abline(v = 50, lwd = 2)
abline(h = 50, lwd = 2)
abline(h = 75, lwd = 1)
abline(v = 75, lwd = 1)
abline(h = 25, lwd = 1)
abline(v = 25, lwd = 1)


```


### Run TSP function
run_TSP() generates the route.  
Sometimes, splitting a cluster into quadrants results in an empty quadrant, so run_TSP() checks for that 
and skips over the empty one.   
Empty quadrants occur because the cluster is split along cardinal directions (N,S,E,W). 
Splitting the cluster along its principal direction might prevent this problem.  
```{r}

# run_TSP() runs the TSP problem over the given point cluster
# @param clust - the point cluster - sp object
# @param close_pnt - the point used to find the start point - coords
# returns - the tour length in miles
run_TSP <- function(clust, close_pnt) {
  print(paste0("clust size ", nrow(clust@coords)))
  # sometimes splitting the cluster returns an empty quadrant
  # if that happens, return a 0 for the length
  if (length(clust@coords) > 0) {
    
    # get the starting point for the TSP, it is closest to the
    d <- pointDistance(close_pnt, clust@coords, lonlat = FALSE)
    start_idx <- which(d == min(d))
    
    # get ETSP object with cluster, solve it, find tour length
    tsp_obj <- as.ETSP(clust@coords)
    tour <- solve_TSP(tsp_obj, method = "nn", control = c(start = start_idx))
    
    # length of tour + distance to travel between quadrants
    tour_dist <- tour_length(tour, tsp_obj)/5280
    tour_dist <- tour_dist + (min(d)/5280)
    
    # return the ending points idx
    tour <- as.integer(tour)   
    end_idx <- tour[length(tour)]
    
    
    #res <- list(start_idx = start_idx, end_idx = end_idx, tour_dist = tour_dist)
    res <- list(end_idx = end_idx, tour_dist = tour_dist)
  } else {

    #res <- list(start_idx = 1, end_idx = NA, tour_dist = 0)
    res <- list(end_idx = NA, tour_dist = 0)
  } # close if else
  return(res)
} # close run_TSP()


```

The run_TSP() function is ran over the points_list in a code loop.  
The individual routes are connected to form a route based on the distance from the end point of one route to the start point of the next. The distance to and from the AD is also added.  
```{r}

 # order the list by their name to get circle to go the right way
points_list <- points_list[order(names(points_list))]

# the first time its ran, start at the point closest to the cluster AD
close2 <- cluster$AD@coords

#start_idx_list <- list()
end_idx_list <- list()
#start_sp <- list()
end_sp <- list()
route_dist <- list()
# run the TSP over the whole thing
for (i in 1:length(points_list)) {

  poop <- run_TSP(clust = points_list[[i]], close_pnt = close2)

  route_dist[[i]] <- poop$tour_dist

  
  # if the quadrant is empty, duplicate the end SP of the previous quadrant
  # if the NW quadrant is empty (first evaluated), end sp is the AD
  # this is a lot of ifs....maybe should have split on PD to prevent empty quadrants??
  if (is.na(poop$end_idx) == TRUE) {
      if (i == 1) {
        end_sp[[i]] <- SpatialPoints(AD_coords, cluster@proj4string)
      } else {
         end_sp[[i]] <- end_sp[[i-1]]
      } # close inner if/else
  } else {
    end_sp[[i]] <- points_list[[i]][poop$end_idx,]
  } # close ifelse

  # if the cluster is empty, the end_idx wil return as NA
  # in that case, dont change the close2 point
  if (is.na(poop$end_idx) == TRUE) {
    close2 <- close2
  } else {
   # reset the close2 variable as the end point
    close2 <- points_list[[i]][poop$end_idx,]
  } # close ifelse

  print(paste0("Tour dist. quadrant ", i, ": ", round(poop$tour_dist, 2), " miles"))
} # close for loop

# add tour lengths together
route_dist <- do.call(sum, route_dist)

# also add the distance from the last stop back to the AD
d <- pointDistance(cluster$AD@coords, end_sp[[length(end_sp)]]@coords, lonlat = FALSE)
route_dist <- route_dist + (d/5280)

print(paste0("Total Collection Route: ", round(route_dist, 2), " miles"))

```


# Estimate GHG emissions
GHG emissions are calculated for a single AD at a time. 
run_TSP() (above) is used to get the VMT of the cluster for collection/hauling emissions  

## Set up constants
```{r}


#~~~~~~~ Unit Conversions ~~~~~~~~~~~#
# collection/hauling distance
cluster$AD$collect_dist_mi <- route_dist

# convert [tons FW] into [kg FW]
cluster$AD$AD_cap_kg <- cluster$AD$AD_cap * 907.185
cluster$AD$FW_dis_kg <- cluster$AD$FW_dis * 907.185


#~~~~~~ Set up constants ~~~~~~~~~~~~#

# nitrogen in feedstock - [kg]
# assuming 17% wet weight is N
## Source: Small Scale AD GHG Calculator spreadsheet (Source: Sara data)
cluster$AD$N_fdstck <- cluster$AD$FW_dis_kg * 0.17

## Food Waste Avoided Landfill Emission Factor (0.39) from [MTCO2e/short ton waste] to [kgCO2e/kg waste]
# Source: CCI Emissions Factor database - Food waste tab
GHG_FLW_prevent <- 0.39 * 1000*2.2/2000

# kg per mile diesel hauling (light, 6,000lbs capacity) - [kg CO2e per kg/mi]
## Source: Small Scale AD GHG Calculator spreadsheet (Source: Sara data)
GHG_fuel_use <- (0.46+(0.00007*296))/2700


```

## Calculate GHG Component Constants
These values are dependent on the amount of FW diverted from landfills and are the same in both scenarios  
```{r}

# avoided landfill gas by FW diversion [kg CO2e/year]
LFG_avoid <- cluster$AD$FW_dis_kg * GHG_FLW_prevent

# nitrogen fertilizer substitution for AD digestate (avoided N production) [kg CO2e/year]
# Source: SmallAD GHG Calculator (Sara data)
# this is not dependent on digestor efficiency, nitrogen in FW does not change
avoid_N_CO2e <-  cluster$AD$N_fdstck * 0.867

# Transport GHGs [kg CO2e/year]
# GHG emissions for a weekly collection route is calculated.
# capacity limitations to the truck is ignored
# all FW on the route (1/52nd of FW_dis) is assumed to travel the entire length of the route
# this is then multipled by 52 to get a yearly collection value for the cluster (FWG is in units of tons/year)
CO2e_collect <-  52 * ((cluster$AD$FW_dis_kg/52) * cluster$AD$collect_dist_mi * GHG_fuel_use)

```


## Scenario 1 (Less Efficient AD)
Scenario 1, all FW goes to digestor, but AD less efficient if overfilled (reduced biogas generation). 
A discount factor that accounts for the faster throughput of FW through the AD is applied to methane generation.
```{r}

# ~~~~~~ From Sara's email ~~~~~~~~~~~
# For non-overfilled digesters, baseline assumptions:
# OLR_base = digester size (55,000 ton; 10,000 ton; 5,000 ton) / 365 days = [ton/days] (OLR - Organic loading rate)
# example:  55,000 ton / 365 days = 150.7 ton/day
# Discount Factor = 1

# Overfilled Digesters:
# OLR_new = tons / 365 days
# example: 65,000 ton/365 days = 178 ton/day
# Discount Factor (DF) = OLR_base / OLR_new
# example:  150.7 ton/day / 178 ton/day = 0.85

# Methane Production Calculation - add discount for all calculations
# AD_cap_kg*0.26*0.45*DF
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  

# Base organic loading rate (daily AD capacity) [kg/day]
OLR_base <- cluster$AD$AD_cap_kg[1] / 365

# Methane generation by AD - [m3]
# Function of volatile solids in food waste
# assume 26% volatile solids (Zhang et al 2007, Fisgativa et al 2016)
# assume  0.45 m3 ch4/kg VS (Bong et al 2018, Fisgativa et al 2016)
# if FW allocated to AD higher than AD capacity, use a discount factor of OLR_base/OLR_new
# otherwise, discount factor is 1
cluster$AD$methane.1 <- ifelse(cluster$AD$FW_dis_kg > cluster$AD$AD_cap_kg, 
                               cluster$AD$FW_dis_kg * 0.26 * 0.45 * (OLR_base / (cluster$AD$FW_dis_kg/365)),
                               cluster$AD$FW_dis_kg * 0.26 * 0.45 * 1)

# natural gas equivalent - [m3]
# assuming 60% methane
cluster$AD$NG_equiv.1 <- cluster$AD$methane.1 * 0.6

# Avoided Natural Gas due to biogas generation
# units: [kg CO2e/year]
# Natural Gas Emissions Factor (53.12/28.32) source: Small_AD_GHG_calculator (Sara Data)
avoid_NG_CO2e.1 = cluster$AD$NG_equiv.1 * (53.12/28.32) 
                            


# Scenario 1 total GHG emissions for the sample cluster
# collection GHGs (the postitive emissions) - all other emissions (GHG credits/negative emissions)
Scen1_GHG_CO2e_kg <- CO2e_collect - LFG_avoid - avoid_N_CO2e - avoid_NG_CO2e.1

print(paste0("Scen 1 GHG for sample cluster: ", round(Scen1_GHG_CO2e_kg, 2), " kg CO2e/year"))
```


## Scenario 2 (Larger AD)
If the AD is allocated FW over its capacity, it is assumed a larger AD is placed there that can run at maximum efficiency.
```{r}

# Same data sources as above

# Methane generation by AD - [m3]
cluster$AD$methane.2 <- cluster$AD$FW_dis_kg * 0.26 * 0.45
  
# natural gas equivalent - [m3]
cluster$AD$NG_equiv.2 <- cluster$AD$methane.2 * 0.6
  
# avoided natural gas production [kg C02e / year]
avoid_NG_CO2e.2 = cluster$AD$NG_equiv.2 * (53.12/28.32)


# Scenario 2 total GHG production
Scen2_GHG_CO2e_kg <- CO2e_collect - LFG_avoid - avoid_N_CO2e - avoid_NG_CO2e.2


print(paste0("Scen 2 GHG for sample cluster: ", round(Scen2_GHG_CO2e_kg, 2), " kg CO2e/year"))


```

